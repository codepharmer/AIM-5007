{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf5357f",
   "metadata": {},
   "source": [
    "**Nosson Weissman** <br>\n",
    "**Problem Set 3** <br>\n",
    "**AIM 5007** <br>\n",
    "**Professor M. Schulte** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "54ca9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "id": "52438723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#just in case I want it later on ...\n",
    "def relu(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0babb3e",
   "metadata": {},
   "source": [
    "**Problem 0**\n",
    "\n",
    "Write a function that takes dimensions as inputs and creates an initial set of weights in a matrix of those \n",
    "dimensions. (For the example above, you will need to create an 8 Ã— 7 matrix for ğ‘Šğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ as well as \n",
    "another 8 Ã— 3 matrix for ğ‘Šâ„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›.) Each entry should be a randomly selected nonzero but small value, \n",
    "say in the range (âˆ’0.5 , 0.5). There is one caveat. The final row of the matrix will hold the weights for \n",
    "that layerâ€™s bias weights, and these need to be the same value for each entry.\n",
    "After each pass (or batch, if we choose a batch update approach), we will be updating this set of \n",
    "weights, so the initialization will only need to be done once for each matrix. Hence, problem zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "id": "54b5c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initializer(dimensions):\n",
    "    weight_matrix = (-0.5 * random.random_sample((dimensions[0]-1, dimensions[1])) + 0.5)\n",
    "    bias_weight = (np.full((1,dimensions[1]),(-0.5*random.random_sample()+0.5)))\n",
    "    weight_matrix = np.insert(weight_matrix, dimensions[0]-1, bias_weight, axis=0)\n",
    "    return weight_matrix\n",
    "# weight_initializer((8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce16439",
   "metadata": {},
   "source": [
    "**Problem 1**\n",
    "\n",
    "Write a function that takes a vector of length ğ‘š as input and returns a vector of length ğ‘š + 1, where \n",
    "the output vector is the original vector plus an extra 1 appended as the final entry. For example, your \n",
    "function, here called â€œappendâ€, should do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "68f04b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_1(vector):\n",
    "    return np.insert(vector[0], len(vector[0]), 1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be05c8",
   "metadata": {},
   "source": [
    "**Problem 2**\n",
    "Given the vector ğ‘“ of inputs, we need to use weights in ğ‘Šğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ to obtain the raw, incoming values for \n",
    "the hidden layer nodes. In other words, we need to calculate:\n",
    "â„ğ‘Ÿğ‘ğ‘¤ = ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘“)\n",
    "ğ‘‡ğ‘Šğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡\n",
    "Write a function that takes the properly appended feature vector ğ‘“ and multiplies its transpose by the \n",
    "weight matrix ğ‘Šğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ to get â„ğ‘Ÿğ‘ğ‘¤. (What are the dimensions of this resulting vector?)\n",
    "Note that you should incorporate your function from problem 1 into the function for problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "ef9f5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_raw(vector, weights):\n",
    "    vector = append_1(vector)\n",
    "    return np.dot(vector, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7b1f1",
   "metadata": {},
   "source": [
    "**Problem 3**\n",
    "We next need to activate the neurons in our hidden layer. This means, loosely, we need:\n",
    "â„ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ğ‘‘ = ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„ğ‘Ÿğ‘ğ‘¤)\n",
    "Write a function that takes a vector of raw values and outputs a vector of transformed values, where the \n",
    "transformation is performed using the sigmoid function on each element of the vector individually.\n",
    "What are the dimensions of the output vector with respect to the input vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "4aab1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate_nodes(layer, activation_function):\n",
    "    return activation_function(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06aa81",
   "metadata": {},
   "source": [
    "**Problem 4**\n",
    "We must begin the backward propagation process by calculating our error. Write a function that takes \n",
    "the final, activated output vector and calculates its error with respect to the true vector of one-hot \n",
    "encodings for this observation. Use the squared error loss function we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "7dc9b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_square_error(Tout_layer, targ_layer):\n",
    "    return 0.5*((targ_layer-Tout_layer)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4b8e5",
   "metadata": {},
   "source": [
    "**Problem 5**\n",
    "Write a function that calculates the gradient of the weight from a hidden node to an output node. In \n",
    "Bramerâ€™s notation, this is ğ‘”(ğ¸, ğ‘Šğ‘—ğ‘˜). Note that you will need to make use of the various quantities that \n",
    "are defined above. Make sure you include all necessary inputs to your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "fd03fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hid_to_out_node_gradient(Tout_k, targ_k, hid_layer):\n",
    "    return ((Tout_k - targ_k) * Tout_k * (1- Tout_k) *hid_layer)[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825cdc2",
   "metadata": {},
   "source": [
    "**Problem 6**\n",
    "Write a function that calculates the gradient of the weight from the bias term to the output nodes. In \n",
    "Bramerâ€™s notation, this is ğ‘”(ğ¸, ğ‘ğ‘–ğ‘ğ‘ ğ‘‚).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "id": "7e5b6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biasO_node_to_out_node_gradient(Tout_layer, targ_layer):\n",
    "    bias_gradient = 0\n",
    "    if len(Tout_layer) != len(targ_layer):\n",
    "        raise Exception('output layer and target layer are not same length, but they should be.\\n \\\n",
    "            Please check the area code and the number and try again!')\n",
    "    for i in range(len(Tout_layer)):\n",
    "        bias_gradient += (Tout_layer[i]-targ_layer[i])*Tout_layer[i]*(1-Tout_layer[i])\n",
    "    return bias_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755e253",
   "metadata": {},
   "source": [
    "**Problem 7**\n",
    "Write a function that calculates the gradient of the weight from an input node to a hidden node. In \n",
    "Bramerâ€™s notation, this is ğ‘”(ğ¸, ğ‘¤ğ‘–ğ‘—)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Was simpler to just do the whole layer at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "id": "13e64dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_hid_node_gradients(input_layer, Thid_layer, Tout_layer, Wout, target_output):\n",
    "    gradients = []\n",
    "    for i in range(len(Thid_layer)):\n",
    "        grad_scalar = 0\n",
    "        Thid_i = Thid_layer[i]\n",
    "        for j in range(len(Tout_layer)):\n",
    "            Tout_j = Tout_layer[j]\n",
    "            targ_j = target_output[j]\n",
    "            grad_scalar += (Tout_j-targ_j)*Tout_j*(1-Tout_j)*Wout.T[j][i]\n",
    "        curr_gradient = (grad_scalar*Thid_i*(1-Thid_i)*input_layer[0])\n",
    "        gradients.append(curr_gradient)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "id": "aea96c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-3.28448805e-04, -6.56897609e-04, -4.92673207e-04, -8.21122012e-05]),\n",
       " array([-0.0040999 , -0.0081998 , -0.00614985, -0.00102498]),\n",
       " array([-1.85123644e-05, -3.70247289e-05, -2.77685467e-05, -4.62809111e-06])]"
      ]
     },
     "execution_count": 1283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.array([[4,8,6,1]])\n",
    "inp_to_hid_w = np.array([np.array([0.1, -0.4, -0.1,.2]),\n",
    "                           np.array([0.3, 0.1, -0.2,.2]),\n",
    "                           np.array([-0.2, 0.2, 0.4,.2])])\n",
    "hid_to_out_w = np.array([np.array([0.5, -0.3, 0.2, .3]),\n",
    "                           np.array([0.2, -0.3, 0.1, .3])])\n",
    "H =  np.append(sigmoid(np.dot(inp_to_hid_w,inp[0])),1)\n",
    "O = sigmoid(np.dot(hid_to_out_w,H))\n",
    "hid_to_out_w = hid_to_out_w.T\n",
    "input_to_hid_node_gradients(inp,H[:-1], O, hid_to_out_w, np.array([0.65, 0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f89266",
   "metadata": {},
   "source": [
    "**Problem 8**\n",
    "Write a function that calculates the gradient of the weight from the bias term to the hidden nodes. In \n",
    "Bramerâ€™s notation, this is ğ‘”(ğ¸, ğ‘ğ‘–ğ‘ğ‘ ğ»)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "b0b4f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biasH_node_to_out_gradient(Thid_layer, Tout_layer, target_layer, hid_to_out_W):\n",
    "    bias_to_out_gradient = 0\n",
    "    if len(Tout_layer) != len(target_layer):\n",
    "        raise Exception('Output layer and target layer are not same length, but they should be.')\n",
    "    for i in range(len(Thid_layer)):\n",
    "        for j in range(len(Tout_layer)):\n",
    "            Thid_i = Thid_layer[i]\n",
    "            Tout_j = Tout_layer[j]\n",
    "            targ_j = target_layer[j]\n",
    "            Wji = hid_to_out_W[j][i]\n",
    "            bias_to_out_gradient += (Tout_j-targ_j)*Tout_j*(1-Tout_j)*Wji*Thid_i*(1-Thid_i)\n",
    "    return bias_to_out_gradient\n",
    "# temp_w =  np.array([np.array([0.5, -0.3, 0.2]),np.array([0.2, -0.3, 0.1])])\n",
    "# biasH_node_to_out_gradient(np.array([0.03916572, 0.73105858, 0.96770454]),np.array([0.5729277,  0.54618944]), np.array([0.65, 0.4]),temp_w)\n",
    "# temp_w[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347e3a0",
   "metadata": {},
   "source": [
    "**Problem 9**\n",
    "Write a function that takes as inputs a learning rate (ğ›¼) and updates the weights for the input-to-hidden-node step. Note that you need to update the bias weights so that it is the same weight for each \n",
    "node. You will probably want to incorporate your functions from problems 7-8 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "id": "213dfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(gradients, old_W, alpha):\n",
    "#     alpha is learning rate\n",
    "    return old_W - alpha*gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a25cad",
   "metadata": {},
   "source": [
    "**Problem 10**\n",
    "Write a function that takes as inputs a learning rate (ğ›¼) and updates the weights for the input-to\u0002hidden-node step. Note that you need to update the bias weights so that it is the same weight for each \n",
    "node. You will probably want to incorporate your functions from problems 5-6 here.\n",
    "<br>\n",
    "Should be the same logic as problem #9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0ba42",
   "metadata": {},
   "source": [
    "**Problem 11**\n",
    "We now should have all the pieces that we need to train our model. Write a function that performs one \n",
    "entire pass based on a single training instance, using your functions above. What should the output(s) of \n",
    "this function be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "id": "b834d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(num, number_of_output_nodes):\n",
    "    if num < 0 or type(num) != int or num > number_of_output_nodes:\n",
    "        raise Exception('Error in one-hot-encoder')\n",
    "    vector = np.array([0]*number_of_output_nodes)\n",
    "    vector[num-1] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "id": "c71a7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_runthrough(seed_data, inp_to_hid_W, hid_to_out_W):\n",
    "    input_layer = np.array([seed_data[:-1]])\n",
    "#while testing function with HW2 data we comment out following code\n",
    "    output_number = int(seed_data[-1])\n",
    "    target_output = np.array(one_hot_encoder(output_number, 3))\n",
    "    #while testing function with HW2 data we use the following code\n",
    "#     input_layer = [seed_data]\n",
    "#     target_output = np.array([0.65, 0.4])\n",
    "\n",
    "    if inp_to_hid_W is None:\n",
    "    #initialize weights if they are not yet set\n",
    "        inp_to_hid_W = weight_initializer((8,7))\n",
    "        hid_to_out_W = weight_initializer((8,3))\n",
    "    \n",
    "    #calculate hidden layer\n",
    "    hid_raw = calculate_raw(input_layer, inp_to_hid_W)\n",
    "    Thid_layer = activate_nodes(hid_raw, sigmoid)\n",
    "    \n",
    "    #calculate output layer\n",
    "    out_raw = calculate_raw(np.array([Thid_layer]), hid_to_out_W)\n",
    "    Tout_layer = activate_nodes(out_raw, sigmoid)\n",
    "    \n",
    "    #calculate error\n",
    "    error = calculate_square_error(Tout_layer.T, target_output)\n",
    "    \n",
    "    #caculate gradient from input to hidden layer\n",
    "    hid_to_out_gradient = []\n",
    "    curr_gradient=None\n",
    "    for i in range(len(Tout_layer)):\n",
    "        Tout_i = Tout_layer[i]\n",
    "        targ_i = target_output[i]\n",
    "        curr_gradient = hid_to_out_node_gradient(Tout_i, targ_i, np.append(Thid_layer,1))\n",
    "        hid_to_out_gradient.append(curr_gradient)\n",
    "    hid_to_out_gradient = np.array(hid_to_out_gradient)\n",
    "\n",
    "    \n",
    "    #cacluate gradient from hidden layer to output\n",
    "    inp_to_hid_gradient = input_to_hid_node_gradients(input_layer, Thid_layer, Tout_layer, hid_to_out_W, target_output)\n",
    "    inp_to_hid_gradient = np.array(inp_to_hid_gradient)\n",
    "    learning_rate = 0.2\n",
    "    \n",
    "    #calculate gradients for biases\n",
    "    bias_hid_to_out_gradient = biasO_node_to_out_node_gradient(Tout_layer, target_output)\n",
    "    bias_inp_to_hid_gradient = biasH_node_to_out_gradient(Thid_layer, Tout_layer, target_output, hid_to_out_W.T)\n",
    "    \n",
    "    #update input layer weights\n",
    "    old_input_bias = inp_to_hid_W[-1]\n",
    "    inp_to_hid_W =  update_weights(inp_to_hid_gradient, inp_to_hid_W[:-1].T, learning_rate)\n",
    "    #update input bias node (we use an array for gradients as we will need it to insert to matrix)\n",
    "    inp_bias_gradient = np.full((1,len(inp_to_hid_gradient)),bias_inp_to_hid_gradient)\n",
    "    updated_inp_bias = update_weights(inp_bias_gradient, old_input_bias,learning_rate)\n",
    "    #insert bias as a row in matrix\n",
    "    inp_to_hid_W = np.insert(inp_to_hid_W,len(inp_to_hid_W.T), updated_inp_bias, axis=1)\n",
    "    \n",
    "    #update output layer weights\n",
    "    old_output_bias = hid_to_out_W[-1]\n",
    "    hid_to_out_W =  update_weights(hid_to_out_gradient, hid_to_out_W[:-1].T, learning_rate)\n",
    "    #update output bias node (we use an array for gradients as we will need it to insert to matrix)\n",
    "    out_bias_gradient = np.full((1,len(hid_to_out_gradient)),bias_hid_to_out_gradient) \n",
    "    updated_out_bias = update_weights(out_bias_gradient, old_output_bias,learning_rate)\n",
    "    #insert bias as a row in matrix\n",
    "    hid_to_out_W = np.insert(hid_to_out_W,hid_to_out_W.shape[1], updated_out_bias, axis=1)\n",
    "    \n",
    "    \n",
    "    #update hidden layer\n",
    "    updated_hid_raw = calculate_raw(input_layer,inp_to_hid_W.T)\n",
    "    updated_hid_layer = activate_nodes(updated_hid_raw, sigmoid)\n",
    "    \n",
    "    #update out layer \n",
    "    updated_out_raw = calculate_raw([updated_hid_layer], hid_to_out_W.T)\n",
    "    updated_out_layer = activate_nodes(updated_out_raw, sigmoid)\n",
    "    \n",
    "    \n",
    "    updated_error = calculate_square_error(updated_out_layer, target_output)\n",
    "    \n",
    "    return [inp_to_hid_W, hid_to_out_W, updated_error]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62977b80",
   "metadata": {},
   "source": [
    "### Test function using data from HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "id": "6bac561c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.10006569, -0.39986862, -0.09990147,  0.20022234],\n",
       "        [ 0.30081998,  0.10163996, -0.19877003,  0.20022234],\n",
       "        [-0.1999963 ,  0.2000074 ,  0.40000555,  0.20022234]]),\n",
       " array([[ 0.50014772, -0.29724271,  0.20364983,  0.29652454],\n",
       "        [ 0.19971616, -0.30529805,  0.09298695,  0.29652454]]),\n",
       " 0.013085241854687813]"
      ]
     },
     "execution_count": 1279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.array([4,8,6])\n",
    "inp_to_hid_w = np.array([[0.1, -0.4, -0.1,.2],\n",
    "                           [0.3, 0.1, -0.2,.2],\n",
    "                           [-0.2, 0.2, 0.4,.2]]).T\n",
    "hid_to_out_w = np.array([[0.5, -0.3, 0.2, .3],\n",
    "                           [0.2, -0.3, 0.1, .3]]).T\n",
    "\n",
    "observation_runthrough(inp, inp_to_hid_w, hid_to_out_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af246b3",
   "metadata": {},
   "source": [
    "### Results are the same as HW2 results!\n",
    "### Seems to be working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49c5d4",
   "metadata": {},
   "source": [
    "**Now lets run through a few examples from the seed dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "id": "3e4aefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 :\n",
      "updated input weights [[0.39661409 0.02753509 0.43652498 0.15796599 0.15574799 0.06055781\n",
      "  0.27215842 0.0450598 ]\n",
      " [0.11942667 0.45816912 0.32870197 0.01557228 0.27334957 0.10251766\n",
      "  0.37962546 0.0450598 ]\n",
      " [0.23636096 0.18686367 0.27599594 0.45329725 0.02443742 0.45243262\n",
      "  0.33196277 0.0450598 ]\n",
      " [0.46558992 0.26928091 0.0927109  0.3889022  0.19431739 0.20236551\n",
      "  0.25382685 0.0450598 ]\n",
      " [0.118869   0.12888567 0.40597336 0.48546387 0.19773813 0.15825583\n",
      "  0.04471445 0.0450598 ]\n",
      " [0.0116222  0.04048346 0.19317153 0.11756358 0.3718603  0.24522554\n",
      "  0.02864536 0.0450598 ]\n",
      " [0.47133857 0.48763665 0.02847995 0.2263927  0.21232464 0.06727957\n",
      "  0.0976889  0.0450598 ]]\n",
      "updated output weights [[0.26313273 0.32021387 0.11260455 0.11160975 0.42315042 0.38999497\n",
      "  0.31903908 0.04186728]\n",
      " [0.42770025 0.36700175 0.19008532 0.46024761 0.01109087 0.25392906\n",
      "  0.06605763 0.04186728]\n",
      " [0.46841387 0.39762765 0.42782609 0.26852642 0.00258614 0.14039722\n",
      "  0.03362723 0.04186728]]\n",
      "error:  0.7613938829490106\n",
      "\n",
      "sample 1 :\n",
      "updated input weights [[0.14205312 0.32880117 0.12895456 0.28335156 0.45051903 0.30799631\n",
      "  0.41225341 0.47541893]\n",
      " [0.41454801 0.38290267 0.030726   0.01529911 0.05603264 0.05407457\n",
      "  0.16272419 0.47541893]\n",
      " [0.02776091 0.02833033 0.2360419  0.33785265 0.45785704 0.13431111\n",
      "  0.08346848 0.47541893]\n",
      " [0.39879999 0.13739082 0.44632903 0.21191414 0.21632603 0.31677981\n",
      "  0.33581532 0.47541893]\n",
      " [0.2113286  0.03427526 0.09011226 0.28168298 0.08134549 0.31483205\n",
      "  0.39888364 0.47541893]\n",
      " [0.48089637 0.15827221 0.27901957 0.46595982 0.14030334 0.10712861\n",
      "  0.31223387 0.47541893]\n",
      " [0.21827361 0.3976576  0.26013176 0.42051825 0.24172459 0.0812064\n",
      "  0.12536626 0.47541893]]\n",
      "updated output weights [[0.45098453 0.2792095  0.35754557 0.19827761 0.05939488 0.15720151\n",
      "  0.22782114 0.41896099]\n",
      " [0.10204735 0.18918856 0.04020537 0.02537205 0.31112928 0.19499109\n",
      "  0.41459707 0.41896099]\n",
      " [0.01008898 0.03697402 0.36694619 0.01601875 0.21602785 0.29401089\n",
      "  0.26478587 0.41896099]]\n",
      "error:  0.711274885116314\n",
      "\n",
      "sample 2 :\n",
      "updated input weights [[0.34275182 0.00255032 0.1887049  0.49787934 0.26155859 0.45374047\n",
      "  0.43011225 0.2985002 ]\n",
      " [0.46368881 0.39263154 0.49527801 0.10581975 0.38623454 0.44829375\n",
      "  0.00469885 0.2985002 ]\n",
      " [0.3610195  0.36514931 0.3027422  0.39675828 0.05529318 0.08231402\n",
      "  0.10844078 0.2985002 ]\n",
      " [0.37448676 0.33444701 0.26097389 0.41362714 0.43129062 0.28373886\n",
      "  0.25572307 0.2985002 ]\n",
      " [0.31182604 0.15631756 0.40213751 0.26096984 0.13112336 0.03377058\n",
      "  0.24194611 0.2985002 ]\n",
      " [0.39623402 0.42806491 0.18669462 0.03069769 0.10391097 0.42180875\n",
      "  0.42846885 0.2985002 ]\n",
      " [0.22274268 0.19496315 0.0118779  0.2080511  0.46794448 0.45703336\n",
      "  0.33795149 0.2985002 ]]\n",
      "updated output weights [[0.07378364 0.02270899 0.39008771 0.3241028  0.2008359  0.23701354\n",
      "  0.18041183 0.40517383]\n",
      " [0.09677237 0.34353976 0.39638995 0.23216727 0.39027641 0.27064192\n",
      "  0.13358249 0.40517383]\n",
      " [0.44422647 0.05808069 0.03865972 0.10107805 0.24046294 0.44275181\n",
      "  0.32503299 0.40517383]]\n",
      "error:  0.8130583783868903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "seeds = None\n",
    "with open('C:/Users/Wesso/Desktop/YU School Stuff/Neural Networks/AIM-5007/seeds_dataset.txt', newline='\\n') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    seeds = [row for row in csv_reader]\n",
    "    for i in range(len(seeds)):\n",
    "        row = seeds[i]\n",
    "        #clean up data and convert to an array\n",
    "        row = np.asarray([datum.strip() for datum in row[0].split('\\t') if datum.strip() != ''], dtype=np.float64)\n",
    "        seeds[i] = row\n",
    "\n",
    "seeds = np.array(seeds)\n",
    "\n",
    "#shuffle the seeds dataset (as it starts off being in order)\n",
    "np.random.shuffle(seeds)\n",
    "\n",
    "#since we're using on ehot encoding, we need to know \n",
    "# the max integer output to determine the number of nodes in the output\n",
    "output_node_count = seeds.max(axis=0)[-1]\n",
    "\n",
    "error_list = []\n",
    "inp_to_hid_W = None\n",
    "hid_to_out_W = None\n",
    "\n",
    "sample_res =[ observation_runthrough(seeds[j], inp_to_hid_W, hid_to_out_W) for j in range(3)]\n",
    "\n",
    "for i, sample  in enumerate(sample_res):\n",
    "    print('sample',i,':')\n",
    "    print('updated input weights', sample[0])\n",
    "    print('updated output weights', sample[1])\n",
    "    print('error: ',sample[2])\n",
    "    print()\n",
    "# for i in range(500):\n",
    "#     for j in range(int(len(seeds)*.8)):\n",
    "#         runthrough_j =  observation_runthrough(seeds[j], inp_to_hid_W, hid_to_out_W)\n",
    "#         error_list.append(runthrough_j[-1])\n",
    "#         inp_to_hid_W = runthrough_i[0]\n",
    "#         hid_to_out_W = runthrough_i[1]\n",
    "# error_list = np.array(error_list)\n",
    "# error_list[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
